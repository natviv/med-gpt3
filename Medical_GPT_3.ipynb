{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natviv/med-gpt3/blob/main/Medical_GPT_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYsx3i8kZK6F"
      },
      "source": [
        "# Medical GPT-3. \n",
        "\n",
        "This colab features an exploration of potential medical applications with GPT-3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5SXT33waILE"
      },
      "source": [
        "This is built using Open AI API's integration with Weights and Biases (W&B)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leuRS_UuabbS"
      },
      "source": [
        "## API key setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-LMVGYzOvgh"
      },
      "outputs": [],
      "source": [
        "# API key credentials\n",
        "%env OPENAI_API_KEY="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "844rRfGfaemZ"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWs9EEwqGb1j",
        "outputId": "6d356b99-5c5e-4ee8-92b6-9214caa92f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.7/dist-packages (0.18.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.16)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.7/dist-packages (from openai) (1.2.0.58)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.9)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.64.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandas-stubs>=1.1.0.11->openai) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2021.10.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "96T5DB0OJmxw"
      },
      "outputs": [],
      "source": [
        "# Setup imports\n",
        "\n",
        "import openai\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "wVBWaQVVIYDw",
        "outputId": "dfaad442-6f21-47ca-af12-c4a87d0fc982"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220515_210157-vsohg1xf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/vsohg1xf\" target=\"_blank\">stellar-elevator-1</a></strong> to <a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(project='Medical GPT-3', job_type=\"dataset_preparation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to dataset in Google Drive"
      ],
      "metadata": {
        "id": "slpjt06ZNUCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_teDrgyTNbMm",
        "outputId": "d810cc17-3534-40f0-89cb-ef9c232056bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read data into pandas csv"
      ],
      "metadata": {
        "id": "vZTAZQ2ZNkrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "df=pd.read_csv('gdrive/MyDrive/med-gpt3-data/data.csv')"
      ],
      "metadata": {
        "id": "uZAt16PKNn4X"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze data"
      ],
      "metadata": {
        "id": "LlSjB1aqi897"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of rows is {len(df)} and number of columns is {len(df.columns)}\")\n",
        "for index, row in df.iterrows():\n",
        "    print(f\"ID: {row['ID']}, Text: {row['Text']}, Completion: {row['Completion']}\")\n",
        "\n",
        "df_test = df.iloc[:5,:]\n",
        "df_train = df.iloc[5:,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg1TR3yPi_vZ",
        "outputId": "93d811f1-515a-4bc7-d14d-6f0203c3f55e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows is 30 and number of columns is 3\n",
            "ID: 1, Text: The pt has lbp, Completion: The patient has lower back pain\n",
            "ID: 2, Text: The pt is a 30 y/o m, Completion: The patient is a 30 year old male\n",
            "ID: 3, Text: VSS after Tx, Completion: Vital signs stable after treatment\n",
            "ID: 4, Text: c/o gi pain, Completion: complains of gastrointestinal pain\n",
            "ID: 5, Text: tx d/c due to c/o h/a, Completion: Treatment discontinued due to compaints of headache\n",
            "ID: 6, Text: abx dosage recommended, Completion: Antibiotics dosage recommended\n",
            "ID: 7, Text: thx required, Completion: therapy required\n",
            "ID: 8, Text: The pt requires lbp pt, Completion: The patient requires lower back pain physical therapy\n",
            "ID: 9, Text: MBC likely impaired due to covid, Completion: Maximum breathing capacity likely impaired due to covid\n",
            "ID: 10, Text: The pt has recurring history of jt pain, Completion: The patient has recurring history of joint pain\n",
            "ID: 11, Text: Nacl low in pt, Completion: Sodium chloride low in patient\n",
            "ID: 12, Text: LOS = 18 days, Completion: Lenght of stay = 18 days\n",
            "ID: 13, Text: LOC low, pt likely in coma, Completion: Level of consciouness low, patient likely in coma\n",
            "ID: 14, Text: PA will recommend next steps, Completion: Physician's Assistant will recommend next steps\n",
            "ID: 15, Text: NSA detected, Completion: No specific abnormality detected\n",
            "ID: 16, Text: The Pt when admitted was PN, Completion: The patient when admitted was pourly nourished\n",
            "ID: 17, Text: PN on duty administered the drugs, Completion: Practical nurse on duty administered the drugs\n",
            "ID: 18, Text: Low o2 sat. detected in pt, Completion: Low oxygen saturation detected in patient\n",
            "ID: 19, Text: PTA, vss were not normal, Completion: Prior to admission, vital signs were not normal\n",
            "ID: 20, Text: UTI as primary cause of fever, Completion: Urinary tract infection as primary cause of fever\n",
            "ID: 21, Text: Pt put on vent, Completion: Patient put on ventilator\n",
            "ID: 22, Text: fx detected in femur, Completion: fracture detected in femur\n",
            "ID: 23, Text: GNA to assist in care of pt, Completion: Geriatric Nurse Assistant to assist in care of patient\n",
            "ID: 24, Text: GCS low at admission, Completion: Glasgow Coma scale low at admission\n",
            "ID: 25, Text: Gt. tr required for pt, Completion: Gait training required for patient\n",
            "ID: 26, Text: Pt dc ama, Completion: Patient discharged against medical advice\n",
            "ID: 27, Text: CXR required to assess tx, Completion: Chest x-ray required to assess treatment\n",
            "ID: 28, Text: PET inconclusive, Completion: positron emission\n",
            "tomography inconclusive\n",
            "ID: 29, Text: oe pt recommended to be placed under obs for 24 hours, Completion: On examination, patient recommended to placed under observation for 24 hours\n",
            "ID: 30, Text: pt dnka, Completion: Patient did not keep appointment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use completion API to see how the 'text-davinci-002' model performs. Try with the following approaches:\n",
        "\n",
        "* Zero shot with only instruction\n",
        "* Few shot in-context learning \n",
        "* Few shot with chain of thought prompting -> https://arxiv.org/abs/2201.11903"
      ],
      "metadata": {
        "id": "CQ1qGm5RmwJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# This can be made more efficient and converted into a few batch calls\n",
        "def get_predictions(df, model, context, temperature=0.1, max_tokens=20, use_qa_prefix=False):\n",
        "  results = []\n",
        "  for _, row in tqdm(df.iterrows()):\n",
        "      input = row['Text'] if not use_qa_prefix else row['Text'] + 'Q:'\n",
        "      prompt = context + input + ' ->'\n",
        "      res = openai.Completion.create(model=model, \n",
        "                                     prompt=prompt, \n",
        "                                     max_tokens=max_tokens, \n",
        "                                     temperature = temperature, \n",
        "                                     stop=[\" END\"])\n",
        "      completion = res['choices'][0]['text']\n",
        "      completion = completion[1:] # remove initial space\n",
        "      results.append(f\"Text: {row['Text']}, Target: {row['Completion']}, Prediction: {completion}\")\n",
        "  return results\n",
        "\n",
        "def print_results(results): \n",
        "  for row in results:\n",
        "    print(f\"\\n{row}\")\n",
        "\n",
        "model = 'text-davinci-002'\n",
        "# Zero-shot with only instruction prompt\n",
        "instruction = 'Convert the doctor note into patient readable format without abbreviations.\\n'\n",
        "results = get_predictions(df_test, model, instruction)\n",
        "print_results(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5SBqIpPmB_A",
        "outputId": "762fbee2-db7d-480f-df6b-204b0abc38ac"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:07,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: The pt has lbp, Target: The patient has lower back pain, Prediction: The patient has low back pain.\n",
            "\n",
            "Text: The pt is a 30 y/o m, Target: The patient is a 30 year old male, Prediction: The patient is a 30 year old male.\n",
            "\n",
            "The patient is a 30 year old male.\n",
            "\n",
            "Text: VSS after Tx, Target: Vital signs stable after treatment, Prediction: VSS (visual acuity) after treatment\n",
            "\n",
            "The patient's visual acuity was 20/\n",
            "\n",
            "Text: c/o gi pain, Target: complains of gastrointestinal pain, Prediction: complaining of gastrointestinal pain\n",
            "\n",
            "Text: tx d/c due to c/o h/a, Target: Treatment discontinued due to compaints of headache, Prediction: \n",
            "The doctor has discharged me from the hospital due to my complaints of headaches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero shot with instruction only prompting seems to generate spurious results particularly beyond the length necessary. Lets see if this can be fixed with some in-context learning examples in addition to the instruction.\n",
        "\n",
        "In the above scenario, only 2/5 are correct and model can be seen rambling along in a couple."
      ],
      "metadata": {
        "id": "VBC7A7ig2fJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = 'Convert the doctor note into patient readable format without abbreviations.\\n'\n",
        "example1 = 'The pt exhibits symptoms of CAD -> The patient exhibits symptoms of Coronary Artery Disease\\n'\n",
        "example2 = 'Pt has prior history of hbp -> Patient has prior history of high blood pressure\\n'\n",
        "\n",
        "# Few-shot in-context learning with instruction prompt and additional few shot examples\n",
        "context = instruction + example1 + example2\n",
        "results = get_predictions(df_test, model, context)\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Qgc32X23lnn",
        "outputId": "e9a0a444-91c7-49ae-e730-dbf648e416b5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:06,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: The pt has lbp, Target: The patient has lower back pain, Prediction: The patient has low back pain\n",
            "\n",
            "Text: The pt is a 30 y/o m, Target: The patient is a 30 year old male, Prediction: The patient is a 30 year old male\n",
            "\n",
            "Text: VSS after Tx, Target: Vital signs stable after treatment, Prediction: Vital Signs Stable after treatment\n",
            "\n",
            "Text: c/o gi pain, Target: complains of gastrointestinal pain, Prediction: complains of gastrointestinal pain\n",
            "\n",
            "The patient exhibits symptoms of Coronary Artery Disease. The patient\n",
            "\n",
            "Text: tx d/c due to c/o h/a, Target: Treatment discontinued due to compaints of headache, Prediction: Treatment was discontinued due to complaint of headache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After in-context learning with only a couple of examples, the model seems to have improved quite significantly. \n",
        "\n",
        "Now the model is able to get 4/5 out of 5 examples correct and the rambling / tendency to generate long sentences seems to have reduced significantly."
      ],
      "metadata": {
        "id": "ujBwg8KIZIax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = 'Convert the doctor note into patient readable format without abbreviations.\\n'\n",
        "example1 = 'Q: The pt exhibits symptoms of CAD -> A: The patient exhibits symptoms of Coronary Artery Disease. Explanation: Here pt stands for patient and CAD stands for Coronary Artery Disease.\\n'\n",
        "example2 = 'Q: Pt has prior history of hbp -> A: Patient has prior history of high blood pressure. Explanation: Here pt stands for patient and hbp stands for high blood pressure.\\n'\n",
        "\n",
        "# Few-shot in-context learning with instruction prompt and additional few shot examples with chain of thought prompting\n",
        "context = instruction + example1 + example2\n",
        "results = get_predictions(df_test, model, context, temperature=0.4, max_tokens=30)\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PBXgZxHZvBK",
        "outputId": "07c254f2-fcf2-4d5e-d8d2-8b97ad487dd2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:07,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: The pt has lbp, Target: The patient has lower back pain, Prediction: The patient has low back pain. Explanation: Here pt stands for patient and lbp stands for low back pain.\n",
            "\n",
            "Text: The pt is a 30 y/o m, Target: The patient is a 30 year old male, Prediction: The patient is a 30 year old male. Explanation: Here pt stands for patient, y/o stands for years old, and m stands for\n",
            "\n",
            "Text: VSS after Tx, Target: Vital signs stable after treatment, Prediction: A: Patient had a heart attack after treatment. Explanation: Here Tx stands for treatment and VSS stands for heart attack.\n",
            "\n",
            "Text: c/o gi pain, Target: complains of gastrointestinal pain, Prediction: complaining of gastrointestinal pain\n",
            "\n",
            "Text: tx d/c due to c/o h/a, Target: Treatment discontinued due to compaints of headache, Prediction: Treatment was discontinued due to complaint of headache.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of thought prompting doesn't seem to be super useful here. Perhaps this is due to the simple nature of the task.\n",
        "\n",
        "However, it is interesting to see inoherent explanations co-relate with in-correct model outputs. This suggests one mechanism to consider using models in medical applications might be to check whether the explanations / proof of work are coherent or not."
      ],
      "metadata": {
        "id": "leuIn4xSdk-x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWbuwCDvUPLl"
      },
      "source": [
        "### Now let's see if finetuning a smaller model can help with improving the performance. \n",
        "\n",
        "I am going to try finetuning with a modest number of examples (20) as this is primarily for demonstration purposes.\n",
        "\n",
        "Using Weights & Biases integration for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "1a78286791ed40689996da9b58f15c3e",
            "59f0b8961fcc4c9abaa68659851ee358",
            "7193a14693384fe78e486a64f90577c5",
            "a0adc210135e4d96bb2c51d5da1b5e69",
            "8ca98a8530934ceebc1340d0812cbb3e",
            "2914a8241cb24336bacea54d9cd37db4",
            "2a56e911d76845058be93e838ed8cca7",
            "2c178b33f42c40f0ba92a7f71508e158"
          ]
        },
        "id": "Q2KYJVLeURpe",
        "outputId": "fbe0e7b9-146a-47ff-e839-b9e96202d5e8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:fru794zn) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a78286791ed40689996da9b58f15c3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">generous-dew-3</strong>: <a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/fru794zn\" target=\"_blank\">https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/fru794zn</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220516_015154-fru794zn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:fru794zn). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220516_015512-3rf8xhqh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/3rf8xhqh\" target=\"_blank\">cool-blaze-4</a></strong> to <a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(project='Medical GPT-3')\n",
        "\n",
        "# artifact = run.use_artifact('/content/gdrive/MyDrive/med-gpt3-data/data.csv', type='raw_dataset')\n",
        "# artifact_dir = artifact.download()+\"/data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WH4JxQ2wnxVe",
        "outputId": "8bb75ddc-6b30-4b19-e6df-a1cd873ec577"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    ID                                             Prompt  \\\n",
              "2    3                                       VSS after Tx   \n",
              "28  29  oe pt recommended to be placed under obs for 2...   \n",
              "13  14                       PA will recommend next steps   \n",
              "10  11                                     Nacl low in pt   \n",
              "26  27                          CXR required to assess tx   \n",
              "\n",
              "                                           Completion  \n",
              "2                  Vital signs stable after treatment  \n",
              "28  On examination, patient recommended to placed ...  \n",
              "13    Physician's Assistant will recommend next steps  \n",
              "10                     Sodium chloride low in patient  \n",
              "26           Chest x-ray required to assess treatment  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a528aeb9-8baf-42f3-8555-2552d8f314a5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Prompt</th>\n",
              "      <th>Completion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>VSS after Tx</td>\n",
              "      <td>Vital signs stable after treatment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>oe pt recommended to be placed under obs for 2...</td>\n",
              "      <td>On examination, patient recommended to placed ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>PA will recommend next steps</td>\n",
              "      <td>Physician's Assistant will recommend next steps</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>Nacl low in pt</td>\n",
              "      <td>Sodium chloride low in patient</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>CXR required to assess tx</td>\n",
              "      <td>Chest x-ray required to assess treatment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a528aeb9-8baf-42f3-8555-2552d8f314a5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a528aeb9-8baf-42f3-8555-2552d8f314a5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a528aeb9-8baf-42f3-8555-2552d8f314a5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "#Shuffling the dataset with fixed seed\n",
        "\n",
        "df = pd.read_csv('gdrive/MyDrive/med-gpt3-data/data.csv')\n",
        "ds = df.sample(frac=1.0, random_state=0)\n",
        "ds.rename(columns={'Text': 'Prompt'}, inplace=True)\n",
        "ds.to_csv(\"data.csv\")\n",
        "ds.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsXVxE95RPS6"
      },
      "source": [
        "### Using OpenAI tools to preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0dY1ieyQAZV",
        "outputId": "e010047c-9c45-4332-8265-0c3cb8483a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Based on your file extension, your file is formatted as a CSV file\n",
            "- Your file contains 30 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
            "- The `prompt` column/key should be lowercase\n",
            "- The `completion` column/key should be lowercase\n",
            "- The input file should contain exactly two columns/keys per row. Additional columns/keys present are: ['Unnamed: 0', 'ID']\n",
            "  WARNING: Some of the additional columns/keys contain `Unnamed: 0` in their name. These will be ignored, and the column/key `Unnamed: 0` will be used instead. This could also result from a duplicate column/key in the provided file.\n",
            "  WARNING: Some of the additional columns/keys contain `ID` in their name. These will be ignored, and the column/key `ID` will be used instead. This could also result from a duplicate column/key in the provided file.\n",
            "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
            "- Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples.\n",
            "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
            "- [Necessary] Lower case column name to `prompt`\n",
            "- [Necessary] Lower case column name to `completion`\n",
            "- [Necessary] Remove additional columns/keys: ['Unnamed: 0', 'ID']\n",
            "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y\n",
            "/usr/local/lib/python3.7/dist-packages/openai/validators.py:215: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x[\"prompt\"] += suffix\n",
            "- [Recommended] Add a suffix ending `.` to all completions [Y/n]: Y\n",
            "/usr/local/lib/python3.7/dist-packages/openai/validators.py:371: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x[\"completion\"] += suffix\n",
            "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
            "/usr/local/lib/python3.7/dist-packages/openai/validators.py:415: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  lambda x: (\"\" if x[0] == \" \" else \" \") + x\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
            "\n",
            "Wrote modified file to `data_prepared.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"data_prepared.jsonl\"\n",
            "\n",
            "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\".\"]` so that the generated texts ends at the expected place.\n",
            "Once your model starts training, it'll approximately take 2.86 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ],
      "source": [
        "!openai tools fine_tunes.prepare_data -f data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ099Vana-b_"
      },
      "source": [
        "### Splitting the data into train and val sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "zAlDkw1MNtvz"
      },
      "outputs": [],
      "source": [
        "# The dataset has 30 examples. We will use 20 for training and 10 for testing\n",
        "\n",
        "!head -n 20 data_prepared.jsonl > train.jsonl\n",
        "!tail -n 10  data_prepared.jsonl > valid.jsonl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "qN0SCMVj43Rp"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4so6FA5FbPlt"
      },
      "source": [
        "### GPT-3 fine-tuning hyper-parameters definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "OhoZ_gFfOhbX"
      },
      "outputs": [],
      "source": [
        "model = 'ada'  # can be ada, babbage or curie\n",
        "n_epochs = 4\n",
        "batch_size = 4\n",
        "learning_rate_multiplier = 0.1\n",
        "prompt_loss_weight = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5gcrsJabSKM"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "0C7PLw2QOs47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b96aa42-f47b-4e4d-c25a-a7a8594aa820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rUpload progress:   0% 0.00/1.91k [00:00<?, ?it/s]\rUpload progress: 100% 1.91k/1.91k [00:00<00:00, 2.68Mit/s]\n",
            "Uploaded file from train.jsonl: file-yJOD9irAl3PoA6Ip4bdF6xnd\n",
            "Upload progress: 100% 995/995 [00:00<00:00, 1.76Mit/s]\n",
            "Uploaded file from valid.jsonl: file-LPVmd5FAgyeEiuEV42Pd5Enb\n",
            "Created fine-tune: ft-0aDcGNgNcG0428ffEg98wQLJ\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2022-05-16 02:05:38] Created fine-tune: ft-0aDcGNgNcG0428ffEg98wQLJ\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-0aDcGNgNcG0428ffEg98wQLJ\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!openai api fine_tunes.create \\\n",
        "    -t train.jsonl \\\n",
        "    -v valid.jsonl \\\n",
        "    -m $model \\\n",
        "    --n_epochs $n_epochs \\\n",
        "    --batch_size $batch_size \\\n",
        "    --learning_rate_multiplier $learning_rate_multiplier \\\n",
        "    --prompt_loss_weight $prompt_loss_weight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.follow -i ft-0aDcGNgNcG0428ffEg98wQLJ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6eVdfLjrjjG",
        "outputId": "509cf292-f2f4-438b-8b5a-873aa8ce435d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-05-16 02:05:38] Created fine-tune: ft-0aDcGNgNcG0428ffEg98wQLJ\n",
            "[2022-05-16 02:12:07] Fine-tune costs $0.00\n",
            "[2022-05-16 02:12:08] Fine-tune enqueued. Queue number: 0\n",
            "[2022-05-16 02:12:10] Fine-tune started\n",
            "[2022-05-16 02:12:28] Completed epoch 1/4\n",
            "[2022-05-16 02:12:31] Completed epoch 2/4\n",
            "[2022-05-16 02:12:33] Completed epoch 3/4\n",
            "[2022-05-16 02:12:36] Completed epoch 4/4\n",
            "[2022-05-16 02:12:55] Uploaded model: ada:ft-personal-2022-05-16-02-12-53\n",
            "[2022-05-16 02:12:58] Uploaded result file: file-UayQnPtF3tbXqyq2mFGwoZ9d\n",
            "[2022-05-16 02:12:58] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-personal-2022-05-16-02-12-53 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeCgcFbVbUhh"
      },
      "source": [
        "## Sync fine-tune jobs to Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cfet6LhbXYZ"
      },
      "source": [
        "Log fine-tuning runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "-ipLXku8P8TF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1894179e-4e0c-4109-bda2-e0a24a3c5d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatviv\u001b[0m (\u001b[33mnatviv-gpt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220516_022438-ft-tVmTevSFiUgcdJdeSAgokoQA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mft-tVmTevSFiUgcdJdeSAgokoQA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-tVmTevSFiUgcdJdeSAgokoQA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss █▄▄▆▄▅▄▃▅▅▄▄▂▃▄▂▄▅▆▅▁▄▃▂▄▅▂▃▂▁▂▃▃▂▃▃▂▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy ▃▂▁▃▄▅▂▄▃▃▃▄▄▅▃▅▅▅▄▅▆▅▆▆▆▄▆▇▅▆█▆█▆▆▆▇▇█▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss ▆▅▄▅▄▅▁▃▄▄▂▂▅▄▃▄▄▆▂▃▅▂▆▂▆▂▂▅▅█▃▅▃▅▃▃█▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy ▄▂▃▃▃▄▂▄▃▃▃▃▄█▄▂▃▅▅▃▃▄▆▅▃▁▃▃▆▃▅▅▄▄▃▁▄▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples 1104.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens 147440.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             fine_tuned_model curie:ft-personal-20...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       status succeeded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss 1.13666\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy 0.55072\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss 1.24751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy 0.4812\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mft-tVmTevSFiUgcdJdeSAgokoQA\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-tVmTevSFiUgcdJdeSAgokoQA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220516_022438-ft-tVmTevSFiUgcdJdeSAgokoQA/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220516_022457-ft-ezdbVJW1Cxsb77ZKJ33d0aZY\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mft-ezdbVJW1Cxsb77ZKJ33d0aZY\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-ezdbVJW1Cxsb77ZKJ33d0aZY\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss █▄▄▆▅▅▄▃▅▅▄▄▂▃▅▂▄▅▅▅▁▄▃▂▄▅▂▃▂▁▂▂▂▂▃▃▂▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy ▃▁▁▃▅▅▂▃▃▃▃▄▄▅▃▅▄▅▄▅▇▆▆▇▆▄▆▇▅▇█▆▇▆▅▇▇▇█▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss ▆▅▄▅▄▅▁▃▄▄▂▂▅▄▃▄▄▆▂▃▅▂▆▂▆▂▂▅▅█▃▅▃▅▂▃█▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy ▄▃▃▃▃▅▃▅▄▄▃▃▄█▅▂▃▅▅▃▃▄▆▅▃▂▃▄▆▂▅▄▅▄▃▁▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples 1104.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens 147440.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             fine_tuned_model curie:ft-personal-20...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       status succeeded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss 1.10288\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy 0.54348\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss 1.27963\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy 0.45489\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mft-ezdbVJW1Cxsb77ZKJ33d0aZY\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-ezdbVJW1Cxsb77ZKJ33d0aZY\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220516_022457-ft-ezdbVJW1Cxsb77ZKJ33d0aZY/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220516_022515-ft-rV9X9BWeR9DxtlnfOA8z489r\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mft-rV9X9BWeR9DxtlnfOA8z489r\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-rV9X9BWeR9DxtlnfOA8z489r\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss ▅█▆▂▅▃▄▃▄▅▄▃▆▃▃▄▄▄▄▃▂▃▃▄▃▂▁▃▁▂▃▃▄▂▅▂▁▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy ▁▂▁▃▃▂▁▂▁▂▃▅▂▄▅▄▄▃▅▅█▅▆▅▄▄▆▆▅▆▇▆█▆▆▇█▆█▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss ▇▃▄▂█▃▄▆▅▃▂▇▆▂▄▄▄▅▆▃▄▅▂▅▅▁▃▃▅▆▅▆▄▃▅▇▅▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy ▄▁▃▇▅▆▇▆▆▇▃▅▆▃▆▆▇▆▇▆▄▅▆▅▆▄▁▆▅█▅▇▅▂▂▄▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples 1104.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens 148496.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             fine_tuned_model ada:ft-personal-2022...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       status succeeded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss 1.15586\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy 0.5604\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss 2.10775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy 0.43404\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mft-rV9X9BWeR9DxtlnfOA8z489r\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-rV9X9BWeR9DxtlnfOA8z489r\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220516_022515-ft-rV9X9BWeR9DxtlnfOA8z489r/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220516_022530-ft-0aDcGNgNcG0428ffEg98wQLJ\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mft-0aDcGNgNcG0428ffEg98wQLJ\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-0aDcGNgNcG0428ffEg98wQLJ\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens ▁▁▂▂▂▃▃▃▄▄▅▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss ▄█▅▆▅▃▃▃▃▂▂▃▁▂▁▁▁▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy ▁▁▁▁▁▁▅▁▁▁▁▁▅▅▁█▁▁▅█▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy ▂▁▂▁▄▆▅▃▅▇▅▆▇▇▇██▇▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss ▆▅█▄▅▃▄▄▃▄▂▂▇▃▄▁▄▅▁▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy ▁▂▁▅▄▆▇▂▆▆▅▇▃▇▅█▄▄█▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples 84.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens 1876.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             fine_tuned_model ada:ft-personal-2022...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       status succeeded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss 0.63487\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy 0.25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy 0.83871\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss 0.75455\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy 0.74286\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mft-0aDcGNgNcG0428ffEg98wQLJ\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-0aDcGNgNcG0428ffEg98wQLJ\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220516_022530-ft-0aDcGNgNcG0428ffEg98wQLJ/logs\u001b[0m\n",
            "🎉 wandb sync completed successfully\n"
          ]
        }
      ],
      "source": [
        "!openai wandb sync --project \"Medical GPT-3\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZb_9oeKiefO"
      },
      "source": [
        "## Run inference on validation examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc6AecHwihJF"
      },
      "source": [
        "Run some predictions on a few validation samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "XH8IUPA7lkFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "9bd86e1a-2029-4f60-92e1-37f0d98d4718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatviv\u001b[0m (\u001b[33mnatviv-gpt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220516_023250-1g6bsbwv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/1g6bsbwv\" target=\"_blank\">chocolate-frog-9</a></strong> to <a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# create eval job\n",
        "run = wandb.init(project='Medical GPT-3', job_type='eval')\n",
        "entity = wandb.run.entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "iXlnqZIZlvh8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "215b6b5d-127c-4e0b-dccc-2d5eefd3d60c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ada:ft-personal-2022-05-16-02-12-53'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "# choose a fine-tuned model\n",
        "artifact_job = run.use_artifact(f'{entity}/Medical GPT-3/fine_tune_details:latest', type='fine_tune_details')\n",
        "artifact_job.metadata\n",
        "\n",
        "wandb.config.update({k:artifact_job.metadata[k] for k in ['fine_tuned_model', 'model', 'hyperparams']})\n",
        "fine_tuned_model = artifact_job.metadata['fine_tuned_model']\n",
        "fine_tuned_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0O6UXXgjgi3"
      },
      "source": [
        "Loading validation data as dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "fpPsEznRmN_e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "77b335b5-58e1-49cc-8c82-8c6df007922e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       prompt  \\\n",
              "0                 PTA, vss were not normal ->   \n",
              "1            UTI as primary cause of fever ->   \n",
              "2  The pt has recurring history of jt pain ->   \n",
              "3                   The pt requires lbp pt ->   \n",
              "4                                Pt dc ama ->   \n",
              "\n",
              "                                          completion  \n",
              "0   Prior to admission, vital signs were not normal.  \n",
              "1   Urinary tract infection as primary cause of f...  \n",
              "2   The patient has recurring history of joint pain.  \n",
              "3   The patient requires lower back pain physical...  \n",
              "4         Patient discharged against medical advice.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f53efa88-cc22-4190-bb65-92d414ee7425\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>completion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PTA, vss were not normal -&gt;</td>\n",
              "      <td>Prior to admission, vital signs were not normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UTI as primary cause of fever -&gt;</td>\n",
              "      <td>Urinary tract infection as primary cause of f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The pt has recurring history of jt pain -&gt;</td>\n",
              "      <td>The patient has recurring history of joint pain.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The pt requires lbp pt -&gt;</td>\n",
              "      <td>The patient requires lower back pain physical...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pt dc ama -&gt;</td>\n",
              "      <td>Patient discharged against medical advice.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f53efa88-cc22-4190-bb65-92d414ee7425')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f53efa88-cc22-4190-bb65-92d414ee7425 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f53efa88-cc22-4190-bb65-92d414ee7425');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "df = pd.read_json(\"valid.jsonl\", orient='records', lines=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq3rsIFRj1r8"
      },
      "source": [
        "Perform inference on 10 validation examples. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "CdYAMa-Nmlo9"
      },
      "outputs": [],
      "source": [
        "n_samples = 10\n",
        "df = df.iloc[:n_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "z5X7_7J6m1Lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1dfd45-6010-4ca5-ff8b-4db2e70f26be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [00:17,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: PTA, vss were not normal, Target: Prior to admission, vital signs were not nor, Prediction: Delayed phase analysis Voluntary action test was not normal.\n",
            "\n",
            "\n",
            "\n",
            "PATI, scales indicated normal physiological tests.\n",
            "\n",
            "\n",
            "\n",
            "DAL\n",
            "\n",
            "Prompt: UTI as primary cause of fever, Target: Urinary tract infection as primary cause of fe, Prediction: Statin User As Principal Cause of Fever.\n",
            "\n",
            "104. When treating primary illness with a grant of continuous use of treatment.\n",
            "\n",
            "105.\n",
            "\n",
            "Prompt: The pt has recurring history of jt pain, Target: The patient has recurring history of joint p, Prediction: patient has recurrent jt pain.\n",
            "\n",
            "Intervention\n",
            "\n",
            "Treatment Plan\n",
            "\n",
            "Instruct patient to adhere to prescription medication and instructions. Do not\n",
            "\n",
            "Prompt: The pt requires lbp pt, Target: The patient requires lower back pain physical ther, Prediction: Trained patient requires lbp.\n",
            "\n",
            "Memory loss disorder (more common) memory patient required memory loss disorder. Memory loss disorder. Memory loss patient\n",
            "\n",
            "Prompt: Pt dc ama, Target: Patient discharged against medical adv, Prediction: Photo fatigued orthopedic patient. Pelvic circumference measurement out of balance. Patient requiring manual balancing. Patient switching to orthopedic intensive care unit\n",
            "\n",
            "Prompt: c/o gi pain, Target: complains of gastrointestinal p, Prediction: Poor postural state. It should be managed surgically or as an outpatient. Questions about prevent or treat pain. Swallowed larger than normal amount of\n",
            "\n",
            "Prompt: The pt has lbp, Target: The patient has lower back p, Prediction: Plated Upland Physician.\n",
            "\n",
            "Arriving New Patient.\n",
            "\n",
            "Patient checking me in. Yes, sir! Good patient\n",
            "\n",
            "Prompt: fx detected in femur, Target: fracture detected in fe, Prediction: Level of detection detected in femur.\n",
            "\n",
            "Called in by: Dr. Cederstrom\n",
            "\n",
            "Results may vary. Imaging results may vary\n",
            "\n",
            "Prompt: The Pt when admitted was PN, Target: The patient when admitted was pourly nouris, Prediction: Prostopancia Patient operated. Patient refuses to leave the room. Patient left the room 6 minutes later. Patient left the room 3 minutes later.\n",
            "\n",
            "Prompt: LOC low, pt likely in coma, Target: Level of consciouness low, patient likely in c, Prediction: Advanced life support low, patient likely in coma. Intensive care emeritus. Cardiac arrest likely in coma. Intensive care attendant likely in coma\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "for _, row in tqdm(df.iterrows()):\n",
        "    prompt = row['prompt']\n",
        "    res = openai.Completion.create(model=fine_tuned_model, prompt=prompt, max_tokens=30, stop=[\" END\"])\n",
        "    completion = res['choices'][0]['text']\n",
        "    completion = completion[1:]       # remove initial space\n",
        "    prompt = prompt[:-3]              # remove \" ->\"\n",
        "    target = row['completion'][1:-4]  # remove initial space and \"END\"\n",
        "    results.append(f\"Prompt: {prompt}, Target: {target}, Prediction: {completion}\")\n",
        "\n",
        "print_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smaller Ada model not so useful in the generation task and the results are a bit all over the place.\n",
        "\n",
        "To try with 'curie' or 'da-vinci' for better results "
      ],
      "metadata": {
        "id": "1bcjmQC8t5a8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWe0ogkUj8nE"
      },
      "source": [
        "Create and log a W&B Table to explore, query & compare model predictions if helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeE_QLPTm37_"
      },
      "outputs": [],
      "source": [
        "# prediction_table = wandb.Table(columns=['prompt', 'target', 'completion'], data=data)\n",
        "# wandb.log({'predictions': prediction_table})\n",
        "wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Medical GPT-3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a78286791ed40689996da9b58f15c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59f0b8961fcc4c9abaa68659851ee358",
              "IPY_MODEL_7193a14693384fe78e486a64f90577c5"
            ],
            "layout": "IPY_MODEL_a0adc210135e4d96bb2c51d5da1b5e69"
          }
        },
        "59f0b8961fcc4c9abaa68659851ee358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ca98a8530934ceebc1340d0812cbb3e",
            "placeholder": "​",
            "style": "IPY_MODEL_2914a8241cb24336bacea54d9cd37db4",
            "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "7193a14693384fe78e486a64f90577c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a56e911d76845058be93e838ed8cca7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c178b33f42c40f0ba92a7f71508e158",
            "value": 1
          }
        },
        "a0adc210135e4d96bb2c51d5da1b5e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ca98a8530934ceebc1340d0812cbb3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2914a8241cb24336bacea54d9cd37db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a56e911d76845058be93e838ed8cca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c178b33f42c40f0ba92a7f71508e158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}