{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natviv/med-gpt3/blob/main/Medical_GPT_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYsx3i8kZK6F"
      },
      "source": [
        "# Medical GPT-3. \n",
        "\n",
        "This colab features an exploration of potential medical applications with GPT-3.\n",
        "\n",
        "The following applications are prototyped and explored.\n",
        "\n",
        "*   Convert clinical notes with abbreviations to patient readable format\n",
        "*   A primary care conversational agent that triages with a patient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5SXT33waILE"
      },
      "source": [
        "This is built using Open AI API's integration with Weights and Biases (W&B)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leuRS_UuabbS"
      },
      "source": [
        "## API key setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-LMVGYzOvgh"
      },
      "outputs": [],
      "source": [
        "# API key credentials\n",
        "%env OPENAI_API_KEY="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "844rRfGfaemZ"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWs9EEwqGb1j",
        "outputId": "3ddf3a47-6b2a-417f-b64d-a98be42a2ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.18.1.tar.gz (42 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▊                        | 10 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 20 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 30 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 727 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Collecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.2.0.58-py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 63.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.64.0)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandas-stubs>=1.1.0.11->openai) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 57.5 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: openai, pathtools\n",
            "  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.18.1-py3-none-any.whl size=53168 sha256=1e4fdb34b58cd282d0735352b51268b02c106f6d67f7bd3c6d25712d2d8f035d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/bf/24/fcdc9d2b81f9c7e565bb2036ec9f7cc930056b829895b3bf48\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=6320f38f4a95d5ff5c22f0f060160da5877abec2f65e2c9719d789ec6bf07454\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built openai pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, pandas-stubs, GitPython, docker-pycreds, wandb, openai\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 openai-0.18.1 pandas-stubs-1.2.0.58 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.16\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "96T5DB0OJmxw"
      },
      "outputs": [],
      "source": [
        "# Setup imports\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "import wandb\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "wVBWaQVVIYDw",
        "outputId": "ba76d952-f971-49a9-9fd5-3915a5613ef9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220518_030446-3gvwbpsq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/3gvwbpsq\" target=\"_blank\">ethereal-elevator-14</a></strong> to <a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(project='Medical GPT-3', job_type=\"dataset_preparation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to dataset in Google Drive"
      ],
      "metadata": {
        "id": "slpjt06ZNUCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_teDrgyTNbMm",
        "outputId": "b07ba977-7304-4197-9e0d-afc0e2324626"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read data into pandas csv"
      ],
      "metadata": {
        "id": "vZTAZQ2ZNkrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('gdrive/MyDrive/med-gpt3-data/med_abbreviations.csv')"
      ],
      "metadata": {
        "id": "uZAt16PKNn4X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze data"
      ],
      "metadata": {
        "id": "LlSjB1aqi897"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of rows is {len(df)} and number of columns is {len(df.columns)}\")\n",
        "for index, row in df.iterrows():\n",
        "    print(f\"ID: {row['ID']}, Text: {row['Text']}, Completion: {row['Completion']}\")\n",
        "\n",
        "df_test = df.iloc[:5,:]\n",
        "df_train = df.iloc[5:,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg1TR3yPi_vZ",
        "outputId": "0f357cc7-ef86-47ac-8910-f3f3e1fcdc82"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows is 30 and number of columns is 3\n",
            "ID: 1, Text: The pt has lbp, Completion: The patient has lower back pain.\n",
            "ID: 2, Text: The pt is a 30 y/o m, Completion: The patient is a 30 year old male.\n",
            "ID: 3, Text: VSS after Tx, Completion: Vital signs stable after treatment.\n",
            "ID: 4, Text: c/o gi pain, Completion: complains of gastrointestinal pain.\n",
            "ID: 5, Text: tx d/c due to c/o h/a, Completion: Treatment discontinued due to compaints of headache.\n",
            "ID: 6, Text: abx dosage recommended, Completion: Antibiotics dosage recommended.\n",
            "ID: 7, Text: thx required, Completion: therapy required.\n",
            "ID: 8, Text: The pt requires lbp pt, Completion: The patient requires lower back pain physical therapy.\n",
            "ID: 9, Text: MBC likely impaired due to covid, Completion: Maximum breathing capacity likely impaired due to covid.\n",
            "ID: 10, Text: The pt has recurring history of jt pain, Completion: The patient has recurring history of joint pain.\n",
            "ID: 11, Text: Nacl low in pt, Completion: Sodium chloride low in patient.\n",
            "ID: 12, Text: LOS = 18 days, Completion: Lenght of stay = 18 days.\n",
            "ID: 13, Text: LOC low; pt likely in coma, Completion: Level of consciouness low; patient likely in coma.\n",
            "ID: 14, Text: PA will recommend next steps, Completion: Physician's Assistant will recommend next steps.\n",
            "ID: 15, Text: NSA detected, Completion: No specific abnormality detected.\n",
            "ID: 16, Text: The Pt when admitted was PN, Completion: The patient when admitted was pourly nourished.\n",
            "ID: 17, Text: PN on duty administered the drugs, Completion: Practical nurse on duty administered the drugs.\n",
            "ID: 18, Text: Low o2 sat. detected in pt, Completion: Low oxygen saturation detected in patient.\n",
            "ID: 19, Text: PTA vss were not normal, Completion: Prior to admission vital signs were not normal.\n",
            "ID: 20, Text: UTI as primary cause of fever, Completion: Urinary tract infection as primary cause of fever.\n",
            "ID: 21, Text: Pt put on vent, Completion: Patient put on ventilator.\n",
            "ID: 22, Text: fx detected in femur, Completion: fracture detected in femur\n",
            "ID: 23, Text: GNA to assist in care of pt, Completion: Geriatric Nurse Assistant to assist in care of patient.\n",
            "ID: 24, Text: GCS low at admission, Completion: Glasgow Coma scale low at admission.\n",
            "ID: 25, Text: Gt. tr required for pt, Completion: Gait training required for patient.\n",
            "ID: 26, Text: Pt dc ama, Completion: Patient discharged against medical advice.\n",
            "ID: 27, Text: CXR required to assess tx, Completion: Chest x-ray required to assess treatment.\n",
            "ID: 28, Text: PET inconclusive, Completion: positron emission tomography inconclusive.\n",
            "ID: 29, Text: oe pt recommended to be placed under obs for 24 hours, Completion: On examination patient recommended to placed under observation for 24 hours.\n",
            "ID: 30, Text: pt dnka, Completion: Patient did not keep appointment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use completion API to see how the 'text-davinci-002' model performs. Try with the following approaches:\n",
        "\n",
        "* Zero shot with only instruction\n",
        "* Few shot in-context learning \n",
        "* Few shot with chain of thought prompting -> https://arxiv.org/abs/2201.11903. Also see https://twitter.com/npew/status/1525900849888866307"
      ],
      "metadata": {
        "id": "CQ1qGm5RmwJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# This can be made more efficient and converted into a few batch calls\n",
        "def get_predictions(df, model, context, temperature=0.1, max_tokens=20, use_qa_prefix=False):\n",
        "  results = []\n",
        "  for _, row in tqdm(df.iterrows()):\n",
        "      input = row['Text'] if not use_qa_prefix else row['Text'] + 'Q:'\n",
        "      prompt = context + input + ' ->'\n",
        "      res = openai.Completion.create(model=model, \n",
        "                                     prompt=prompt, \n",
        "                                     max_tokens=max_tokens, \n",
        "                                     temperature = temperature, \n",
        "                                     stop=[\" END\"])\n",
        "      completion = res['choices'][0]['text']\n",
        "      completion = completion[1:] # remove initial space\n",
        "      results.append(f\"Text: {row['Text']}, Target: {row['Completion']}, Prediction: {completion}\")\n",
        "  return results\n",
        "\n",
        "def print_results(results): \n",
        "  for row in results:\n",
        "    print(f\"\\n{row}\")\n",
        "\n",
        "model = 'text-davinci-002'\n",
        "# Zero-shot with only instruction prompt\n",
        "instruction = 'Convert the doctor note into patient readable format without abbreviations.\\n'\n",
        "results = get_predictions(df_test, model, instruction)\n",
        "print_results(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5SBqIpPmB_A",
        "outputId": "c47f3b39-93d6-46d7-fee9-cc9e2c7f6d04"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:07,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: The pt has lbp, Target: The patient has lower back pain., Prediction: The patient has low back pain.\n",
            "\n",
            "Text: The pt is a 30 y/o m, Target: The patient is a 30 year old male., Prediction: The patient is a 30 year old male.\n",
            "\n",
            "The patient is a 30 year old male.\n",
            "\n",
            "Text: VSS after Tx, Target: Vital signs stable after treatment., Prediction: VSS (visual acuity) after treatment\n",
            "\n",
            "The patient's visual acuity was 20/\n",
            "\n",
            "Text: c/o gi pain, Target: complains of gastrointestinal pain., Prediction: complaining of gastrointestinal pain\n",
            "\n",
            "Text: tx d/c due to c/o h/a, Target: Treatment discontinued due to compaints of headache., Prediction: \n",
            "The doctor has discharged me from the hospital due to my complaints of headaches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero shot with instruction only prompting seems to generate spurious results particularly beyond the length necessary. Lets see if this can be fixed with some in-context learning examples in addition to the instruction.\n",
        "\n",
        "In the above scenario, only 2/5 are correct and model can be seen rambling along in a couple."
      ],
      "metadata": {
        "id": "VBC7A7ig2fJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = 'Convert the doctor note into patient readable format without abbreviations.\\n'\n",
        "example1 = 'The pt exhibits symptoms of CAD -> The patient exhibits symptoms of Coronary Artery Disease\\n'\n",
        "example2 = 'Pt has prior history of hbp -> Patient has prior history of high blood pressure\\n'\n",
        "\n",
        "# Few-shot in-context learning with instruction prompt and additional few shot examples\n",
        "context = instruction + example1 + example2\n",
        "results = get_predictions(df_test, model, context)\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Qgc32X23lnn",
        "outputId": "b5fd3c68-8af8-4690-d50f-41bad61a07bb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:07,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: The pt has lbp, Target: The patient has lower back pain., Prediction: The patient has low back pain\n",
            "\n",
            "Text: The pt is a 30 y/o m, Target: The patient is a 30 year old male., Prediction: The patient is a 30 year old male\n",
            "\n",
            "Text: VSS after Tx, Target: Vital signs stable after treatment., Prediction: Vital Signs Stable after treatment\n",
            "\n",
            "Text: c/o gi pain, Target: complains of gastrointestinal pain., Prediction: complains of gastrointestinal pain\n",
            "\n",
            "The patient exhibits symptoms of Coronary Artery Disease. The patient\n",
            "\n",
            "Text: tx d/c due to c/o h/a, Target: Treatment discontinued due to compaints of headache., Prediction: Treatment was discontinued due to complaint of headache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After in-context learning with only a couple of examples, the model seems to have improved quite significantly. \n",
        "\n",
        "Now the model is able to get 4/5 out of 5 examples correct and the rambling / tendency to generate long sentences seems to have reduced significantly."
      ],
      "metadata": {
        "id": "ujBwg8KIZIax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = 'Convert the doctor note into patient readable format without abbreviations.\\n'\n",
        "example1 = 'Q: The pt exhibits symptoms of CAD -> A: The patient exhibits symptoms of Coronary Artery Disease. Explanation: Here pt stands for patient and CAD stands for Coronary Artery Disease.\\n'\n",
        "example2 = 'Q: Pt has prior history of hbp -> A: Patient has prior history of high blood pressure. Explanation: Here pt stands for patient and hbp stands for high blood pressure.\\n'\n",
        "\n",
        "# Few-shot in-context learning with instruction prompt and additional few shot examples with chain of thought prompting\n",
        "context = instruction + example1 + example2\n",
        "results = get_predictions(df_test, model, context, temperature=0.4, max_tokens=30)\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PBXgZxHZvBK",
        "outputId": "448bd3d9-1c33-4679-cdb0-f1778d55da85"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:06,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: The pt has lbp, Target: The patient has lower back pain., Prediction: A: The patient has low back pain. Explanation: Here pt stands for patient and lbp stands for low back pain.\n",
            "\n",
            "Text: The pt is a 30 y/o m, Target: The patient is a 30 year old male., Prediction: The patient is a 30 year old male. Explanation: Here pt stands for patient, y/o stands for years old, and m stands for\n",
            "\n",
            "Text: VSS after Tx, Target: Vital signs stable after treatment., Prediction: A: VSS after treatment. Explanation: Here Tx stands for treatment.\n",
            "\n",
            "Text: c/o gi pain, Target: complains of gastrointestinal pain., Prediction: Complains of gastrointestinal pain\n",
            "\n",
            "Text: tx d/c due to c/o h/a, Target: Treatment discontinued due to compaints of headache., Prediction: Treatment was discontinued due to complaint of headache.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of thought prompting doesn't seem to be super useful here. Perhaps this is due to the simple nature of the task.\n",
        "\n",
        "However, it is interesting to see incoherent explanations co-relate with in-correct model outputs. This suggests one mechanism to consider using models in medical applications might be to check whether the explanations / proof of work are coherent or not."
      ],
      "metadata": {
        "id": "leuIn4xSdk-x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWbuwCDvUPLl"
      },
      "source": [
        "### Now let's see if finetuning a smaller model can help with improving the performance. \n",
        "\n",
        "Trying finetuning with a modest number of examples (20) as this is primarily for demonstration purposes.\n",
        "\n",
        "Using Weights & Biases integration for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "a224918aa16e4f45985eb18706a88955",
            "1bc0912b5d1d4eee90ddd607810a2d68",
            "269ef534bd064e0e98f094b779ddb7e2",
            "66ac930f00944d429ece194eca0b1144",
            "fb0e061f2fa443ccae429db4dc73e4f2",
            "9b35852dc2e64d5e8aba75480854b48a",
            "5d272b3f431a4e09a2a1f29abda96626",
            "d172ea2736864d6a8ded15ac796ba909"
          ]
        },
        "id": "Q2KYJVLeURpe",
        "outputId": "fe66bfa6-ca91-4be3-ab4e-d0c32bb89425"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:35y4bapx) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a224918aa16e4f45985eb18706a88955"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">pretty-snowflake-15</strong>: <a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/35y4bapx\" target=\"_blank\">https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/35y4bapx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220518_040050-35y4bapx/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:35y4bapx). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220518_041040-30hm1hts</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/30hm1hts\" target=\"_blank\">dauntless-glade-16</a></strong> to <a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(project='Medical GPT-3')\n",
        "\n",
        "# artifact = run.use_artifact('/content/gdrive/MyDrive/med-gpt3-data/med_abbreviations.csv', type='raw_dataset')\n",
        "# artifact_dir = artifact.download()+\"/data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WH4JxQ2wnxVe",
        "outputId": "69727955-7295-4629-d877-6d7b9a45e813"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    ID                                             Prompt  \\\n",
              "2    3                                       VSS after Tx   \n",
              "28  29  oe pt recommended to be placed under obs for 2...   \n",
              "13  14                       PA will recommend next steps   \n",
              "10  11                                     Nacl low in pt   \n",
              "26  27                          CXR required to assess tx   \n",
              "\n",
              "                                           Completion  \n",
              "2                 Vital signs stable after treatment.  \n",
              "28  On examination patient recommended to placed u...  \n",
              "13   Physician's Assistant will recommend next steps.  \n",
              "10                    Sodium chloride low in patient.  \n",
              "26          Chest x-ray required to assess treatment.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a389f3ef-260a-4be1-8c96-7014c06f1464\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Prompt</th>\n",
              "      <th>Completion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>VSS after Tx</td>\n",
              "      <td>Vital signs stable after treatment.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>oe pt recommended to be placed under obs for 2...</td>\n",
              "      <td>On examination patient recommended to placed u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>PA will recommend next steps</td>\n",
              "      <td>Physician's Assistant will recommend next steps.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>Nacl low in pt</td>\n",
              "      <td>Sodium chloride low in patient.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>CXR required to assess tx</td>\n",
              "      <td>Chest x-ray required to assess treatment.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a389f3ef-260a-4be1-8c96-7014c06f1464')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a389f3ef-260a-4be1-8c96-7014c06f1464 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a389f3ef-260a-4be1-8c96-7014c06f1464');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "#Shuffling the dataset with fixed seed\n",
        "\n",
        "df = pd.read_csv('gdrive/MyDrive/med-gpt3-data/med_abbreviations.csv')\n",
        "ds = df.sample(frac=1.0, random_state=0)\n",
        "ds.rename(columns={'Text': 'Prompt'}, inplace=True)\n",
        "ds.to_csv(\"data.csv\")\n",
        "ds.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsXVxE95RPS6"
      },
      "source": [
        "### Using OpenAI tools to preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0dY1ieyQAZV",
        "outputId": "1805b91d-d3fa-41d9-f295-efc880f9d965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Based on your file extension, your file is formatted as a CSV file\n",
            "- Your file contains 30 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
            "- The `prompt` column/key should be lowercase\n",
            "- The `completion` column/key should be lowercase\n",
            "- The input file should contain exactly two columns/keys per row. Additional columns/keys present are: ['Unnamed: 0', 'ID']\n",
            "  WARNING: Some of the additional columns/keys contain `Unnamed: 0` in their name. These will be ignored, and the column/key `Unnamed: 0` will be used instead. This could also result from a duplicate column/key in the provided file.\n",
            "  WARNING: Some of the additional columns/keys contain `ID` in their name. These will be ignored, and the column/key `ID` will be used instead. This could also result from a duplicate column/key in the provided file.\n",
            "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
            "- Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples.\n",
            "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
            "- [Necessary] Lower case column name to `prompt`\n",
            "- [Necessary] Lower case column name to `completion`\n",
            "- [Necessary] Remove additional columns/keys: ['Unnamed: 0', 'ID']\n",
            "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y\n",
            "/usr/local/lib/python3.7/dist-packages/openai/validators.py:215: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x[\"prompt\"] += suffix\n",
            "- [Recommended] Add a suffix ending `\\n` to all completions [Y/n]: Y\n",
            "/usr/local/lib/python3.7/dist-packages/openai/validators.py:371: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x[\"completion\"] += suffix\n",
            "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
            "/usr/local/lib/python3.7/dist-packages/openai/validators.py:415: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  lambda x: (\"\" if x[0] == \" \" else \" \") + x\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
            "\n",
            "Wrote modified file to `data_prepared (1).jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"data_prepared (1).jsonl\"\n",
            "\n",
            "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"\\n\"]` so that the generated texts ends at the expected place.\n",
            "Once your model starts training, it'll approximately take 2.86 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ],
      "source": [
        "!openai tools fine_tunes.prepare_data -f data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ099Vana-b_"
      },
      "source": [
        "### Splitting the data into train and val sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zAlDkw1MNtvz"
      },
      "outputs": [],
      "source": [
        "# The dataset has 30 examples. We will use 20 for training and 10 for testing\n",
        "\n",
        "!head -n 20 data_prepared.jsonl > train.jsonl\n",
        "!tail -n 10  data_prepared.jsonl > valid.jsonl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qN0SCMVj43Rp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "0de343a205b847a680d1d174bc359f14",
            "648cf4819666425a8c33826836c982ec",
            "3a7a1774fb2a45d08af79e3b0be1fc6f",
            "4230de5645e34f36b49afeae8f6c3d91",
            "d68b615f57a74e5b9e2864d3079af381",
            "3baade64fcce43bbae07d356b0cf97cc",
            "4a0393cafa964cef85595d2c4493f96b",
            "c6c1367705ac4b29a149bf95bbbcfe86"
          ]
        },
        "outputId": "51488a71-b466-428e-e968-10de8b636e40"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0de343a205b847a680d1d174bc359f14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">dauntless-glade-16</strong>: <a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/30hm1hts\" target=\"_blank\">https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/30hm1hts</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220518_041040-30hm1hts/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4so6FA5FbPlt"
      },
      "source": [
        "### GPT-3 fine-tuning 'ada' hyper-parameters definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OhoZ_gFfOhbX"
      },
      "outputs": [],
      "source": [
        "model = 'ada'  # can be ada, babbage or curie\n",
        "n_epochs = 10\n",
        "batch_size = 4\n",
        "learning_rate_multiplier = 0.05\n",
        "prompt_loss_weight = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5gcrsJabSKM"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0C7PLw2QOs47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b0bcce-c46a-4feb-db19-4b586b989c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found potentially duplicated files with name 'train.jsonl', purpose 'fine-tune' and size 1910 bytes\n",
            "file-v1pWf4Rzd6epNVT5eSwehFXZ\n",
            "file-yJOD9irAl3PoA6Ip4bdF6xnd\n",
            "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: \n",
            "Upload progress: 100% 1.91k/1.91k [00:00<00:00, 818kit/s]\n",
            "Uploaded file from train.jsonl: file-bqnPlyL9koVLWf0sCOUsxXFC\n",
            "Found potentially duplicated files with name 'valid.jsonl', purpose 'fine-tune' and size 995 bytes\n",
            "file-LPVmd5FAgyeEiuEV42Pd5Enb\n",
            "file-Ex2NITSRAYxTYkkqotDvNmHT\n",
            "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: \n",
            "Upload progress: 100% 995/995 [00:00<00:00, 1.03Mit/s]\n",
            "Uploaded file from valid.jsonl: file-YPL77TwQe8YwFRxleUfLGJNm\n",
            "Created fine-tune: ft-eP8h1UV1obmolYmIPQFniTXA\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2022-05-18 04:12:45] Created fine-tune: ft-eP8h1UV1obmolYmIPQFniTXA\n",
            "[2022-05-18 04:15:00] Fine-tune costs $0.00\n",
            "[2022-05-18 04:15:00] Fine-tune enqueued. Queue number: 0\n",
            "[2022-05-18 04:15:03] Fine-tune started\n",
            "[2022-05-18 04:15:21] Completed epoch 1/10\n",
            "[2022-05-18 04:15:24] Completed epoch 2/10\n",
            "[2022-05-18 04:15:26] Completed epoch 3/10\n",
            "[2022-05-18 04:15:29] Completed epoch 4/10\n",
            "[2022-05-18 04:15:31] Completed epoch 5/10\n",
            "[2022-05-18 04:15:34] Completed epoch 6/10\n",
            "[2022-05-18 04:15:37] Completed epoch 7/10\n",
            "[2022-05-18 04:15:39] Completed epoch 8/10\n",
            "[2022-05-18 04:15:42] Completed epoch 9/10\n",
            "[2022-05-18 04:15:45] Completed epoch 10/10\n",
            "[2022-05-18 04:16:09] Uploaded model: ada:ft-personal-2022-05-18-04-16-07\n",
            "[2022-05-18 04:16:12] Uploaded result file: file-pNSHjUMi6XVoJPuaHVbo5IAy\n",
            "[2022-05-18 04:16:12] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-personal-2022-05-18-04-16-07 -p <YOUR_PROMPT>\n"
          ]
        }
      ],
      "source": [
        "!openai api fine_tunes.create \\\n",
        "    -t train.jsonl \\\n",
        "    -v valid.jsonl \\\n",
        "    -m $model \\\n",
        "    --n_epochs $n_epochs \\\n",
        "    --batch_size $batch_size \\\n",
        "    --learning_rate_multiplier $learning_rate_multiplier \\\n",
        "    --prompt_loss_weight $prompt_loss_weight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.follow -i ft-eP8h1UV1obmolYmIPQFniTXA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6eVdfLjrjjG",
        "outputId": "7ab30cb7-01ac-4d10-bbc3-a84aa1346b1a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-05-18 04:12:45] Created fine-tune: ft-eP8h1UV1obmolYmIPQFniTXA\n",
            "[2022-05-18 04:15:00] Fine-tune costs $0.00\n",
            "[2022-05-18 04:15:00] Fine-tune enqueued. Queue number: 0\n",
            "[2022-05-18 04:15:03] Fine-tune started\n",
            "[2022-05-18 04:15:21] Completed epoch 1/10\n",
            "[2022-05-18 04:15:24] Completed epoch 2/10\n",
            "[2022-05-18 04:15:26] Completed epoch 3/10\n",
            "[2022-05-18 04:15:29] Completed epoch 4/10\n",
            "[2022-05-18 04:15:31] Completed epoch 5/10\n",
            "[2022-05-18 04:15:34] Completed epoch 6/10\n",
            "[2022-05-18 04:15:37] Completed epoch 7/10\n",
            "[2022-05-18 04:15:39] Completed epoch 8/10\n",
            "[2022-05-18 04:15:42] Completed epoch 9/10\n",
            "[2022-05-18 04:15:45] Completed epoch 10/10\n",
            "[2022-05-18 04:16:09] Uploaded model: ada:ft-personal-2022-05-18-04-16-07\n",
            "[2022-05-18 04:16:12] Uploaded result file: file-pNSHjUMi6XVoJPuaHVbo5IAy\n",
            "[2022-05-18 04:16:12] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-personal-2022-05-18-04-16-07 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeCgcFbVbUhh"
      },
      "source": [
        "## Sync fine-tune jobs to Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cfet6LhbXYZ"
      },
      "source": [
        "Log fine-tuning runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-ipLXku8P8TF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34fe15a-07bd-4181-b470-bb211129ba9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatviv\u001b[0m (\u001b[33mnatviv-gpt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220518_041708-ft-eP8h1UV1obmolYmIPQFniTXA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mft-eP8h1UV1obmolYmIPQFniTXA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-eP8h1UV1obmolYmIPQFniTXA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss ▆▅█▅▆▃▄▄▄▃▃▂▃▃▃▂▂▂▂▂▁▂▂▁▁▁▂▁▂▁▁▁▂▁▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁▃▁▃▃▁▃▃▃▅▅▆▃▅▆▅▆█▆█▆▅▆▅▆██▆██▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy ▂▂▁▁▃▅▄▅▄▅▅▅▆▄▅▇▇▅▇▇█▇▇█▇▇████▇█▇██████▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss ▇▅██▅▄▄▅▄▃▃▅▆▂▁▄▆▁▄▁▅▂▂▃▄▅▄▁▄▁▅▁▂▃▃▅▄▆▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy ▁▁▂▃▃▃▅▃▄▆▆▄▄▇▆▃▃▆▃▇▃▆▇▆▅▄▆█▆▇▅█▇▇▆▇▆▄▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples 204.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens 4588.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             fine_tuned_model ada:ft-personal-2022...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       status succeeded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss 0.29799\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy 0.9375\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss 0.97623\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy 0.72973\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mft-eP8h1UV1obmolYmIPQFniTXA\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-eP8h1UV1obmolYmIPQFniTXA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220518_041708-ft-eP8h1UV1obmolYmIPQFniTXA/logs\u001b[0m\n",
            "🎉 wandb sync completed successfully\n"
          ]
        }
      ],
      "source": [
        "!openai wandb sync --project \"Medical GPT-3\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZb_9oeKiefO"
      },
      "source": [
        "## Run inference on validation examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc6AecHwihJF"
      },
      "source": [
        "Run some predictions on a few validation samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XH8IUPA7lkFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "93dcc9b3-e57d-4fab-a188-9cd54fe926ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatviv\u001b[0m (\u001b[33mnatviv-gpt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220518_041823-elxt40zf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/elxt40zf\" target=\"_blank\">devout-brook-18</a></strong> to <a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# create eval job\n",
        "run = wandb.init(project='Medical GPT-3', job_type='eval')\n",
        "entity = wandb.run.entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "iXlnqZIZlvh8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "60040009-143a-44f6-b112-2c640810b920"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ada:ft-personal-2022-05-18-04-16-07'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# choose a fine-tuned model\n",
        "artifact_job = run.use_artifact(f'{entity}/Medical GPT-3/fine_tune_details:latest', type='fine_tune_details')\n",
        "artifact_job.metadata\n",
        "\n",
        "wandb.config.update({k:artifact_job.metadata[k] for k in ['fine_tuned_model', 'model', 'hyperparams']})\n",
        "fine_tuned_model = artifact_job.metadata['fine_tuned_model']\n",
        "fine_tuned_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0O6UXXgjgi3"
      },
      "source": [
        "Loading validation data as dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "fpPsEznRmN_e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "139fb462-1f7e-4361-d834-d39fdb5906c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       prompt  \\\n",
              "0                 PTA, vss were not normal ->   \n",
              "1            UTI as primary cause of fever ->   \n",
              "2  The pt has recurring history of jt pain ->   \n",
              "3                   The pt requires lbp pt ->   \n",
              "4                                Pt dc ama ->   \n",
              "\n",
              "                                          completion  \n",
              "0   Prior to admission, vital signs were not normal.  \n",
              "1   Urinary tract infection as primary cause of f...  \n",
              "2   The patient has recurring history of joint pain.  \n",
              "3   The patient requires lower back pain physical...  \n",
              "4         Patient discharged against medical advice.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26e06a35-b8fd-4585-bb05-543a5c50093e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>completion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PTA, vss were not normal -&gt;</td>\n",
              "      <td>Prior to admission, vital signs were not normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UTI as primary cause of fever -&gt;</td>\n",
              "      <td>Urinary tract infection as primary cause of f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The pt has recurring history of jt pain -&gt;</td>\n",
              "      <td>The patient has recurring history of joint pain.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The pt requires lbp pt -&gt;</td>\n",
              "      <td>The patient requires lower back pain physical...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pt dc ama -&gt;</td>\n",
              "      <td>Patient discharged against medical advice.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26e06a35-b8fd-4585-bb05-543a5c50093e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-26e06a35-b8fd-4585-bb05-543a5c50093e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-26e06a35-b8fd-4585-bb05-543a5c50093e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "df = pd.read_json(\"valid.jsonl\", orient='records', lines=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq3rsIFRj1r8"
      },
      "source": [
        "Perform inference on 10 validation examples. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CdYAMa-Nmlo9"
      },
      "outputs": [],
      "source": [
        "n_samples = 10\n",
        "df = df.iloc[:n_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "z5X7_7J6m1Lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5457dd88-d2db-4908-e985-1dbb840e45f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [00:06,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: PTA, vss were not normal, Target: Prior to admission, vital signs were not normal., Prediction: Train skills abilities test were high normal. Physically active was normal. Was fit was normal. Social skills skills normal. Knowledge of language was normal.\n",
            "\n",
            "Prompt: UTI as primary cause of fever, Target: Urinary tract infection as primary cause of fever., Prediction: infection as primary cause of fever.\n",
            "\n",
            "Moderate injury as primary cause of fever.\n",
            "\n",
            "Rehabilitation of fever.\n",
            "\n",
            "Hyperther\n",
            "\n",
            "Prompt: The pt has recurring history of jt pain, Target: The patient has recurring history of joint pain., Prediction: The physician has repeated history of subjective pain and it flashes intermittently intermittently (2). Can the patient be treated with non-steroidal anti-\n",
            "\n",
            "Prompt: The pt requires lbp pt, Target: The patient requires lower back pain physical therapy., Prediction: arge patient required. Please tell nurse. (no staff present) Please direct exhausted patient to facility. (staff dont exist) Please direct patient to patient\n",
            "\n",
            "Prompt: Pt dc ama, Target: Patient discharged against medical advice., Prediction: Hand crook down-against X-clock. Raise arms above your head, crook from downward bench, chair or standing – ama. L\n",
            "\n",
            "Prompt: c/o gi pain, Target: complains of gastrointestinal pain., Prediction: Small section of patient's abdomen. Practitioner notified. Minor damage. Well physicians observed. Practitioner advised minor injuries. Patient safely treated.\n",
            "\n",
            "Prompt: The pt has lbp, Target: The patient has lower back pain., Prediction: Patient has a clinical PH balance. Patient has no flowing blood. Patient has low flow rate. Patient has low flow rate. Patient has high flow rate\n",
            "\n",
            "Prompt: fx detected in femur, Target: fracture detected in femur., Prediction: treatment response detected in femur. Treatment ceased after 8 hr. Ulcer healed in 8 hr.\n",
            "Samsung electro-osseous abscess in\n",
            "\n",
            "Prompt: The Pt when admitted was PN, Target: The patient when admitted was pourly nourished., Prediction: Patient when admitted was not in fit condition. Patient discharged from Hospital after admission was not in fit condition. Patient discharged from Hospital after admission was not in\n",
            "\n",
            "Prompt: LOC low, pt likely in coma, Target: Level of consciouness low, patient likely in coma., Prediction: Low patient's Cartoonist(physician) low, patient likely in coma. at this time, patient might be moved to softer position. at this\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "for _, row in tqdm(df.iterrows()):\n",
        "    prompt = row['prompt']\n",
        "    res = openai.Completion.create(model=fine_tuned_model, prompt=prompt, max_tokens=30, stop=[\" END\"])\n",
        "    completion = res['choices'][0]['text']\n",
        "    completion = completion[1:]       # remove initial space\n",
        "    prompt = prompt[:-3]              # remove \" ->\"\n",
        "    target = row['completion'][1:]  # remove initial space\n",
        "    results.append(f\"Prompt: {prompt}, Target: {target}, Prediction: {completion}\")\n",
        "\n",
        "print_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion - smaller Ada model seems not so useful in the generation task and the results are a bit all over the place.\n",
        "\n",
        "A more expansive hyper parameter sweep might help fix this. Another option is to try fine-tuning with the larger 'curie' or 'da-vinci' models for better results "
      ],
      "metadata": {
        "id": "1bcjmQC8t5a8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWe0ogkUj8nE"
      },
      "source": [
        "Create and log a W&B Table to explore, query & compare model predictions if helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BeE_QLPTm37_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "4ad19fe743974b9eb14ca14c5043f3ae",
            "9248577091ef4e7eaf2cf694a138b17a",
            "366f77677afa410b8082ac6bef036ea5",
            "ecb07e5bd6f94f66a2f7bdef6464e528",
            "73b490e3587943d288ce7fbd0b6f3208",
            "e09f8faa96024d8e829a8cb6ff41b502",
            "e1cea02dd7d04581beb5ca65332f53eb",
            "21077bbb79c84098b4ddb25a4576bc19"
          ]
        },
        "outputId": "f7ae740e-d090-4d7c-e143-8f236e20003d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ad19fe743974b9eb14ca14c5043f3ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">devout-brook-18</strong>: <a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/elxt40zf\" target=\"_blank\">https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/elxt40zf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220518_041823-elxt40zf/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# prediction_table = wandb.Table(columns=['prompt', 'target', 'completion'], data=data)\n",
        "# wandb.log({'predictions': prediction_table})\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Now let's try finetuning with the curie model instead to see if it might help###"
      ],
      "metadata": {
        "id": "_9cQzDtVZhEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = 'curie'  # can be ada, babbage or curie\n",
        "n_epochs = 10\n",
        "batch_size = 4\n",
        "learning_rate_multiplier = 0.05\n",
        "prompt_loss_weight = 0.1"
      ],
      "metadata": {
        "id": "MLFT1RBXZsNf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create \\\n",
        "    -t train.jsonl \\\n",
        "    -v valid.jsonl \\\n",
        "    -m $model \\\n",
        "    --n_epochs $n_epochs \\\n",
        "    --batch_size $batch_size \\\n",
        "    --learning_rate_multiplier $learning_rate_multiplier \\\n",
        "    --prompt_loss_weight $prompt_loss_weight\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3ncXNOaZ1pb",
        "outputId": "4b05459b-b217-4842-cfbd-60950ee4df15"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found potentially duplicated files with name 'train.jsonl', purpose 'fine-tune' and size 1910 bytes\n",
            "file-yJOD9irAl3PoA6Ip4bdF6xnd\n",
            "file-v1pWf4Rzd6epNVT5eSwehFXZ\n",
            "file-bqnPlyL9koVLWf0sCOUsxXFC\n",
            "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: \n",
            "Upload progress: 100% 1.91k/1.91k [00:00<00:00, 752kit/s]\n",
            "Uploaded file from train.jsonl: file-hLDjkwyB4ou3Amdq2GgEzs5x\n",
            "Found potentially duplicated files with name 'valid.jsonl', purpose 'fine-tune' and size 995 bytes\n",
            "file-LPVmd5FAgyeEiuEV42Pd5Enb\n",
            "file-YPL77TwQe8YwFRxleUfLGJNm\n",
            "file-Ex2NITSRAYxTYkkqotDvNmHT\n",
            "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: \n",
            "Upload progress: 100% 995/995 [00:00<00:00, 1.03Mit/s]\n",
            "Uploaded file from valid.jsonl: file-zyAlFBzfJz6xu3eYpoFIu8UW\n",
            "Created fine-tune: ft-O2QU3ssy2dxUFjhst2VRqrWE\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2022-05-18 04:24:23] Created fine-tune: ft-O2QU3ssy2dxUFjhst2VRqrWE\n",
            "[2022-05-18 04:26:20] Fine-tune costs $0.01\n",
            "[2022-05-18 04:26:21] Fine-tune enqueued. Queue number: 0\n",
            "[2022-05-18 04:26:23] Fine-tune started\n",
            "[2022-05-18 04:27:16] Completed epoch 1/10\n",
            "[2022-05-18 04:27:20] Completed epoch 2/10\n",
            "[2022-05-18 04:27:23] Completed epoch 3/10\n",
            "[2022-05-18 04:27:27] Completed epoch 4/10\n",
            "[2022-05-18 04:27:30] Completed epoch 5/10\n",
            "[2022-05-18 04:27:34] Completed epoch 6/10\n",
            "[2022-05-18 04:27:38] Completed epoch 7/10\n",
            "[2022-05-18 04:27:41] Completed epoch 8/10\n",
            "[2022-05-18 04:27:45] Completed epoch 9/10\n",
            "[2022-05-18 04:27:48] Completed epoch 10/10\n",
            "[2022-05-18 04:28:11] Uploaded model: curie:ft-personal-2022-05-18-04-28-09\n",
            "[2022-05-18 04:28:14] Uploaded result file: file-OLYpH04cO0AARMQpsabxY3Iu\n",
            "[2022-05-18 04:28:14] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m curie:ft-personal-2022-05-18-04-28-09 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.follow -i ft-O2QU3ssy2dxUFjhst2VRqrWE\n",
        "\n",
        "!openai wandb sync --project \"Medical GPT-3\" \n",
        "\n",
        "# create eval job\n",
        "run = wandb.init(project='Medical GPT-3', job_type='eval')\n",
        "entity = wandb.run.entity\n",
        "\n",
        "# choose a fine-tuned model\n",
        "artifact_job = run.use_artifact(f'{entity}/Medical GPT-3/fine_tune_details:latest', type='fine_tune_details')\n",
        "artifact_job.metadata\n",
        "\n",
        "wandb.config.update({k:artifact_job.metadata[k] for k in ['fine_tuned_model', 'model', 'hyperparams']})\n",
        "fine_tuned_model = artifact_job.metadata['fine_tuned_model']\n",
        "fine_tuned_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9UOk3Wv0aQZf",
        "outputId": "8f8b9a32-6e29-422c-b766-a1cbbc43930d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-05-18 04:24:23] Created fine-tune: ft-O2QU3ssy2dxUFjhst2VRqrWE\n",
            "[2022-05-18 04:26:20] Fine-tune costs $0.01\n",
            "[2022-05-18 04:26:21] Fine-tune enqueued. Queue number: 0\n",
            "[2022-05-18 04:26:23] Fine-tune started\n",
            "[2022-05-18 04:27:16] Completed epoch 1/10\n",
            "[2022-05-18 04:27:20] Completed epoch 2/10\n",
            "[2022-05-18 04:27:23] Completed epoch 3/10\n",
            "[2022-05-18 04:27:27] Completed epoch 4/10\n",
            "[2022-05-18 04:27:30] Completed epoch 5/10\n",
            "[2022-05-18 04:27:34] Completed epoch 6/10\n",
            "[2022-05-18 04:27:38] Completed epoch 7/10\n",
            "[2022-05-18 04:27:41] Completed epoch 8/10\n",
            "[2022-05-18 04:27:45] Completed epoch 9/10\n",
            "[2022-05-18 04:27:48] Completed epoch 10/10\n",
            "[2022-05-18 04:28:11] Uploaded model: curie:ft-personal-2022-05-18-04-28-09\n",
            "[2022-05-18 04:28:14] Uploaded result file: file-OLYpH04cO0AARMQpsabxY3Iu\n",
            "[2022-05-18 04:28:14] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m curie:ft-personal-2022-05-18-04-28-09 -p <YOUR_PROMPT>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatviv\u001b[0m (\u001b[33mnatviv-gpt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220518_042843-ft-O2QU3ssy2dxUFjhst2VRqrWE\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mft-O2QU3ssy2dxUFjhst2VRqrWE\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-O2QU3ssy2dxUFjhst2VRqrWE\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss █▅▆▆▆▇▅▅▃▄▃▄▃▃▄▃▄▄▂▁▃▂▂▂▂▂▂▂▂▂▁▂▃▂▁▁▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁▃▁▃▁▁▁▃▁▁▆▁▃▁▆▃▃▆▃▆▃█▃▃▃▆▆█▃▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy ▁▄▃▄▅▃▄▅▆▅▆▆▇▅▆▆▆▆▇▇▆▇▇▇▇▆▇▇▇▇█▆▇▇█▇█▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss ▇▆▆▆█▆▅▅▅▆▅▄▄▄▆▅▄▃▃▂▄▂▆▃▂▅▂▃▂▄▂▃▁▄▂▄▄▄▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy ▁▂▂▂▂▃▄▄▄▆▄▅▆▅▄▅▇▆█▅▆▆▅▇▆▆▆▆▇▆█▅█▆▆▆▅▆██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples 204.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens 4556.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             fine_tuned_model curie:ft-personal-20...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       status succeeded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss 0.36306\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy 0.92593\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss 0.62106\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy 0.81818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mft-O2QU3ssy2dxUFjhst2VRqrWE\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/ft-O2QU3ssy2dxUFjhst2VRqrWE\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220518_042843-ft-O2QU3ssy2dxUFjhst2VRqrWE/logs\u001b[0m\n",
            "🎉 wandb sync completed successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220518_042900-3ikdu4rb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3/runs/3ikdu4rb\" target=\"_blank\">rosy-firefly-20</a></strong> to <a href=\"https://wandb.ai/natviv-gpt/Medical%20GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'curie:ft-personal-2022-05-18-04-28-09'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform inference\n",
        "results = []\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "for _, row in tqdm(df.iterrows()):\n",
        "    prompt = row['prompt']\n",
        "    res = openai.Completion.create(model=fine_tuned_model, prompt=prompt, max_tokens=30, stop=[\" END\"])\n",
        "    completion = res['choices'][0]['text']\n",
        "    completion = completion[1:]       # remove initial space\n",
        "    prompt = prompt[:-3]              # remove \" ->\"\n",
        "    target = row['completion'][1:]  # remove initial space\n",
        "    results.append(f\"Prompt: {prompt}, Target: {target}, Prediction: {completion}\")\n",
        "\n",
        "print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_w596-1an0u",
        "outputId": "ee5d3175-2424-4b1b-d86b-11e961590579"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [00:22,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: PTA, vss were not normal, Target: Prior to admission, vital signs were not normal., Prediction: PTA, vs were not normal. Patient was diagnosed with possible ventricular tachycardia. Vital signs were not abnormal. The patient was diagnosed\n",
            "\n",
            "Prompt: UTI as primary cause of fever, Target: Urinary tract infection as primary cause of fever., Prediction: CHRONIC CONDITION CAUSED HYPER-CATE-GASTRIC ISCHEMIA as the primary causes of fever.\n",
            "\n",
            "Prompt: The pt has recurring history of jt pain, Target: The patient has recurring history of joint pain., Prediction: Patient has recurrent history of jt pain. 0.4. Decreased Range of Motion. Patient has decreased range of motion. 9. Treatment Plan\n",
            "\n",
            "Prompt: The pt requires lbp pt, Target: The patient requires lower back pain physical therapy., Prediction: Physical Therapy required. Patient requires physical therapy. ( Codeset : G0702 Amelioration of Error ) Medication. Patient required Physio\n",
            "\n",
            "Prompt: Pt dc ama, Target: Patient discharged against medical advice., Prediction: Can't do my dear. Pt cannot excrete today. Ama.\n",
            "\n",
            "\n",
            "* * *\n",
            "\n",
            "\n",
            "Smell sweet/a pinch of sea\n",
            "\n",
            "Prompt: c/o gi pain, Target: complains of gastrointestinal pain., Prediction: Patient with hypertension. Patient currently using a prescription.\n",
            "\n",
            "McKinley, Jeffrey M. Oceanside California. Pain Management, Pain Management Specialist\n",
            "\n",
            "Prompt: The pt has lbp, Target: The patient has lower back pain., Prediction: The patient has anemia.\n",
            "\n",
            "Rock fell on the patient.\n",
            "\n",
            "8/14/10 Mrs G. B. has been admitted for\n",
            "\n",
            "Prompt: fx detected in femur, Target: fracture detected in femur., Prediction: Bone tender detected in femur. Patient alert detected in femur. Degenerative Joint Disease detected in femur. Patient alert detected in femur\n",
            "\n",
            "Prompt: The Pt when admitted was PN, Target: The patient when admitted was pourly nourished., Prediction: Patient toileter when admitted.\n",
            "\n",
            "Rx care is advisable.\n",
            "\n",
            "At the time of assessment of a patient, Pt advised of the need\n",
            "\n",
            "Prompt: LOC low, pt likely in coma, Target: Level of consciouness low, patient likely in coma., Prediction: Patient stable, possible in coma. E. coli bacteria diagnosis.\n",
            "\n",
            "St. Luke's: Suspected viral gastroenteritis.\n",
            "\n",
            "St\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Again the results after finetuning even with the curie model are not very good. This requires further probing and analysis. Perhaps the dataset size is not enough or more longer training / more hyper parameter search is needed."
      ],
      "metadata": {
        "id": "rHoRitfFblas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next, we will prototype a simple conversational primary care agent with GPT-3\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://singularityhub.com/wp-content/uploads/2019/02/doctor-robot-modern-future-health-artificial-intelligence-shutterstock-1072509989-1068x601.jpg' style=\"width:10px;height:20px\" />\n",
        "<figcaption></figcaption></center>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "LcBEDmrDvOL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt for the conversational AI model\n",
        "This can do with more prompt engineering but seems to work decently for starters."
      ],
      "metadata": {
        "id": "nuTXkXqNcLvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "instruction = 'In the following interactions, you are supposed to interact with a user who may have a primary care concern and provide them with support. This support can include triaging, providing a differential diagnosis if certain or help with scheduling a doctor appointment. Please find some example conversations below. When prompted, you are to only provide the doctor\\'s response.'\n",
        "\n",
        "# Provide a simple example of doctor-patient conversation. Not sure how long of a context GPT-3 can handle effectively at the moment\n",
        "example1 = \"Doctor: How can I help? Patient: I have a rash on my skin. Doctor: Anything else? Patient: No Doctor:Is it hurting Patient: Yes, It is swollen and itchy Doctor: Ok, I will refer you to a specialist dermatologist.\"\n",
        "example2 = \"Doctor: How can I help? Patient: I have fever and headache. Doctor: For how long, have you had these symptoms? Patient: 1 day. Doctor: Please come over to the clinic if possible so we can take a closer look.\"\n",
        "\n",
        "prompt = '\\n'.join([instruction, example1, example2])\n"
      ],
      "metadata": {
        "id": "lydbzvhYzls6"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the above prompt actually causes GPT-3 to generate the full dialogue instead of the next turn only\n",
        "\n",
        "Reducing max_tokens to prevent the model from rambling."
      ],
      "metadata": {
        "id": "mmTRpwiF8Pij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "class DoctorAgent(object):\n",
        "  ''' Simple Doctor Agent class '''\n",
        "  def __init__(self):\n",
        "    self._model = 'text-davinci-002'\n",
        "    self._doctor_prefix = 'Doctor: '\n",
        "    self._patient_prefix = 'Patient: '\n",
        "    self._prompt = ''\n",
        "    self._last_doctor_response = 'Hi, I am Dr. AI. How can I help you today? Please say Done to end conversation'\n",
        "    self._last_patient_response = ''\n",
        "    self._max_tokens=100\n",
        "    self._temperature=0.2\n",
        "    self._stop=[\" END\"]\n",
        "    self._stop_agent=False\n",
        "    self._num_turns=0\n",
        "\n",
        "  def send_doctor_response(self):\n",
        "    ''' Print last doctor response '''\n",
        "    print(self._doctor_prefix + '\\n' + self._last_doctor_response)\n",
        "    return\n",
        "  \n",
        "  def get_patient_input(self):\n",
        "    ''' Get next patient input '''\n",
        "    self._last_patient_response = input(f'{self._patient_prefix}\\n')\n",
        "    if self._last_patient_response in ['Done', 'done']:\n",
        "      self._stop_agent=True\n",
        "    self._num_turns += 1\n",
        "    return\n",
        "\n",
        "  def stop_conversation(self):\n",
        "    ''' Check if we should proceed to next turn or not. Limit to 10 turns '''\n",
        "    return self._stop_agent or self._num_turns > 10 \n",
        "\n",
        "  def update_prompt(self, prompt=None):\n",
        "    ''' Update prompt with last conversation turn '''\n",
        "    if prompt is not None:\n",
        "      self._prompt = prompt\n",
        "    else:\n",
        "      self._prompt = ' '.join([self._prompt, self._last_doctor_response, self._patient_prefix, self._last_patient_response]) \n",
        "\n",
        "  def run_model(self):\n",
        "    ''' Run completion model and update last doctor response '''\n",
        "    self._prompt = ' '.join([self._prompt, self._doctor_prefix]) \n",
        "    model_response = openai.Completion.create(model=self._model, \n",
        "                                              prompt=self._prompt, \n",
        "                                              max_tokens=self._max_tokens, \n",
        "                                              temperature=self._temperature, \n",
        "                                              stop=self._stop)\n",
        "    self._last_doctor_response = model_response['choices'][0]['text']\n",
        "    self._last_doctor_response = self._last_doctor_response[1:] # remove initial space"
      ],
      "metadata": {
        "id": "hiooSbm31_cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a doctor agent and run it\n",
        "doctor_agent = DoctorAgent()\n",
        "doctor_agent.update_prompt(prompt)\n",
        "\n",
        "while(True):\n",
        "  doctor_agent.send_doctor_response()\n",
        "  doctor_agent.get_patient_input()\n",
        "  if (doctor_agent.stop_conversation()):\n",
        "    break\n",
        "  doctor_agent.update_prompt()\n",
        "  doctor_agent.run_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XXaH9jvq1ZZ",
        "outputId": "e36d540f-c4b5-4828-b718-422e8993f193"
      },
      "execution_count": 53,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doctor: \n",
            "Hi, I am Dr. AI. How can I help you today? Please say Done to end conversation\n",
            "Patient: \n",
            "I am feeling very tired\n",
            "Doctor: \n",
            "Anything else?\n",
            "Patient: \n",
            "I am not feeling hungry\n",
            "Doctor: \n",
            "Ok, I will refer you to a specialist.\n",
            "Patient: \n",
            "Can you help with an appoitment?\n",
            "Doctor: \n",
            "Yes, I can help you schedule an appointment with your primary care physician.\n",
            "Patient: \n",
            "At what time\n",
            "Doctor: \n",
            "The earliest available appointment is at 2:00 PM. Would that work for you?\n",
            "Patient: \n",
            "yes, where are they located?\n",
            "Doctor: \n",
            "They are located at 123 Main Street.\n",
            "Patient: \n",
            "can you also provide a referral letter?\n",
            "Doctor: \n",
            "Yes, I can provide you with a referral letter.\n",
            "Patient: \n",
            "thanks\n",
            "Doctor: \n",
            "You're welcome.\n",
            "Patient: \n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the above conversation is pretty realistic and a good starting point. The model has a tendency to ramble but this can be fixed hopefully with finetuning on some realistic primary care conversations."
      ],
      "metadata": {
        "id": "1ZTrz28DqHa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other things to prototype with the API are similar notes / patient records retrieval (this has use important applications in clinican settings) using embeddings and search. "
      ],
      "metadata": {
        "id": "hpSLZqkhqcIj"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Medical GPT-3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a224918aa16e4f45985eb18706a88955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bc0912b5d1d4eee90ddd607810a2d68",
              "IPY_MODEL_269ef534bd064e0e98f094b779ddb7e2"
            ],
            "layout": "IPY_MODEL_66ac930f00944d429ece194eca0b1144"
          }
        },
        "1bc0912b5d1d4eee90ddd607810a2d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb0e061f2fa443ccae429db4dc73e4f2",
            "placeholder": "​",
            "style": "IPY_MODEL_9b35852dc2e64d5e8aba75480854b48a",
            "value": "0.017 MB of 0.017 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "269ef534bd064e0e98f094b779ddb7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d272b3f431a4e09a2a1f29abda96626",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d172ea2736864d6a8ded15ac796ba909",
            "value": 1
          }
        },
        "66ac930f00944d429ece194eca0b1144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb0e061f2fa443ccae429db4dc73e4f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b35852dc2e64d5e8aba75480854b48a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d272b3f431a4e09a2a1f29abda96626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d172ea2736864d6a8ded15ac796ba909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0de343a205b847a680d1d174bc359f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_648cf4819666425a8c33826836c982ec",
              "IPY_MODEL_3a7a1774fb2a45d08af79e3b0be1fc6f"
            ],
            "layout": "IPY_MODEL_4230de5645e34f36b49afeae8f6c3d91"
          }
        },
        "648cf4819666425a8c33826836c982ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d68b615f57a74e5b9e2864d3079af381",
            "placeholder": "​",
            "style": "IPY_MODEL_3baade64fcce43bbae07d356b0cf97cc",
            "value": "0.012 MB of 0.012 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "3a7a1774fb2a45d08af79e3b0be1fc6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a0393cafa964cef85595d2c4493f96b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6c1367705ac4b29a149bf95bbbcfe86",
            "value": 1
          }
        },
        "4230de5645e34f36b49afeae8f6c3d91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d68b615f57a74e5b9e2864d3079af381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3baade64fcce43bbae07d356b0cf97cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a0393cafa964cef85595d2c4493f96b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6c1367705ac4b29a149bf95bbbcfe86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ad19fe743974b9eb14ca14c5043f3ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9248577091ef4e7eaf2cf694a138b17a",
              "IPY_MODEL_366f77677afa410b8082ac6bef036ea5"
            ],
            "layout": "IPY_MODEL_ecb07e5bd6f94f66a2f7bdef6464e528"
          }
        },
        "9248577091ef4e7eaf2cf694a138b17a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73b490e3587943d288ce7fbd0b6f3208",
            "placeholder": "​",
            "style": "IPY_MODEL_e09f8faa96024d8e829a8cb6ff41b502",
            "value": "0.013 MB of 0.013 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "366f77677afa410b8082ac6bef036ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1cea02dd7d04581beb5ca65332f53eb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21077bbb79c84098b4ddb25a4576bc19",
            "value": 1
          }
        },
        "ecb07e5bd6f94f66a2f7bdef6464e528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73b490e3587943d288ce7fbd0b6f3208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e09f8faa96024d8e829a8cb6ff41b502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1cea02dd7d04581beb5ca65332f53eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21077bbb79c84098b4ddb25a4576bc19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}